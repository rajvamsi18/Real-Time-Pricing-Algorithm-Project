{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2669146,"sourceType":"datasetVersion","datasetId":55151},{"sourceId":9896478,"sourceType":"datasetVersion","datasetId":6078755}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**PRE MERGE QUALITY CHECKS**\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef check_missing_values(df):\n    \"\"\"Returns columns with missing values and their counts.\"\"\"\n    missing = df.isnull().sum()\n    return missing[missing > 0]\n\ndef check_data_types(df):\n    \"\"\"Returns the data types of each column.\"\"\"\n    return df.dtypes\n\ndef check_duplicates(df):\n    \"\"\"Returns the number of duplicate rows.\"\"\"\n    return df.duplicated().sum()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:24:51.086968Z","iopub.execute_input":"2024-11-13T15:24:51.087915Z","iopub.status.idle":"2024-11-13T15:24:51.094154Z","shell.execute_reply.started":"2024-11-13T15:24:51.087872Z","shell.execute_reply":"2024-11-13T15:24:51.093186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def handle_missing_values(df, strategy='drop', fill_value=None):\n    \"\"\"\n    Handles missing values in the DataFrame.\n    - strategy: 'drop' to remove rows with missing values, 'fill' to impute.\n    - fill_value: Value to fill missing data if strategy is 'fill'.\n    \"\"\"\n    \n    if strategy == 'drop':\n        return df.dropna()\n    elif strategy == 'fill' and fill_value is not None:\n        return df.fillna(fill_value)\n    else:\n        raise ValueError(\"Invalid strategy or fill_value not provided.\")\n\ndef convert_data_types(df, conversions):\n    \"\"\"\n    Converts columns to specified data types.\n    - conversions: Dictionary with column names as keys and target data types as values.\n    \"\"\"\n    for col, dtype in conversions.items():\n        df[col] = df[col].astype(dtype, errors='ignore')\n    return df\n\ndef remove_duplicates(df):\n    \"\"\"Removes duplicate rows from the DataFrame.\"\"\"\n    return df.drop_duplicates()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:24:54.030439Z","iopub.execute_input":"2024-11-13T15:24:54.031057Z","iopub.status.idle":"2024-11-13T15:24:54.037798Z","shell.execute_reply.started":"2024-11-13T15:24:54.031018Z","shell.execute_reply":"2024-11-13T15:24:54.036920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of datasets with their respective file paths\ndatasets = {\n    'Orders': '/kaggle/input/brazilian-ecommerce/olist_orders_dataset.csv',\n    'Order Items': '/kaggle/input/brazilian-ecommerce/olist_order_items_dataset.csv',\n    'Order Payments': '/kaggle/input/brazilian-ecommerce/olist_order_payments_dataset.csv',\n    'Order Reviews': '/kaggle/input/brazilian-ecommerce/olist_order_reviews_dataset.csv',\n    'Products': '/kaggle/input/brazilian-ecommerce/olist_products_dataset.csv',\n    'Sellers': '/kaggle/input/brazilian-ecommerce/olist_sellers_dataset.csv',\n    'Category Translation': '/kaggle/input/brazilian-ecommerce/product_category_name_translation.csv'\n}\n\n# Dictionary to store cleaned DataFrames\ncleaned_dataframes = {}\n\n# Iterate over each dataset\nfor name, file_path in datasets.items():\n    print(f\"Processing {name} dataset...\")\n    \n    # Load the dataset\n    df = pd.read_csv(file_path)\n\n    # Get the shape of the DataFrame\n    rows, columns = df.shape\n    print(f\"The dataset contains {rows} rows and {columns} columns.\")\n    \n    # Perform quality checks\n    missing_values = check_missing_values(df)\n    data_types = check_data_types(df)\n    duplicate_count = check_duplicates(df)\n    \n    # Display quality check results\n    print(f\"Missing Values:\\n{missing_values}\\n\")\n    print(f\"Data Types:\\n{data_types}\\n\")\n    print(f\"Number of Duplicates: {duplicate_count}\\n\")\n    \n    # Data Cleaning Steps\n    # Handle missing values\n\n    # Drop specific columns in \"Order Reviews\" dataset with high missing values\n    if name == 'Order Reviews':\n        columns_to_drop = ['review_comment_title', 'review_comment_message']\n        # Drop only if they exist in the dataset\n        missing_columns = [col for col in columns_to_drop if col in missing_values and missing_values[col] > 0]\n        if missing_columns:\n            print(f\"Dropping columns with high missing values in {name}: {missing_columns}\")\n            df.drop(missing_columns, axis=1, inplace=True)\n    elif not missing_values.empty:\n        # Example: Drop rows with missing values\n        df = handle_missing_values(df, strategy='drop')\n    \n    # Convert data types\n    # Example: Convert date columns to datetime\n    date_columns = [col for col in df.columns if 'date' in col]\n    conversions = {col: 'datetime64[ns]' for col in date_columns}\n    df = convert_data_types(df, conversions)\n    \n    # Remove duplicates\n    if duplicate_count > 0:\n        df = remove_duplicates(df)\n    \n    # Store the cleaned DataFrame\n    cleaned_dataframes[name] = df\n    print(f\"{name} dataset cleaned and stored.\\n\")\n\n\n    #RE-CHECK-------------------------\n\n    # Get the shape of the DataFrame\n    rows, columns = df.shape\n    print(f\"The dataset contains {rows} rows and {columns} columns.\")\n    \n    # Perform quality checks\n    missing_values = check_missing_values(df)\n    data_types = check_data_types(df)\n    duplicate_count = check_duplicates(df)\n    \n    # Display quality check results\n    print(f\"Missing Values:\\n{missing_values}\\n\")\n    print(f\"Data Types:\\n{data_types}\\n\")\n    print(f\"Number of Duplicates: {duplicate_count}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:24:56.992620Z","iopub.execute_input":"2024-11-13T15:24:56.993001Z","iopub.status.idle":"2024-11-13T15:25:00.241592Z","shell.execute_reply.started":"2024-11-13T15:24:56.992966Z","shell.execute_reply":"2024-11-13T15:25:00.240597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cleaned dataframes\nkeys = cleaned_dataframes.keys()\nprint(\"Keys in 'cleaned_dataframes':\", keys)\n\n# Verify the first few rows of each cleaned dataset\nfor name, df in cleaned_dataframes.items():\n    print(f\"First few rows of {name} dataset:\")\n    print(df.head(), \"\\n\")\n\n# Optionally, save cleaned datasets to new CSV files\nfor name, df in cleaned_dataframes.items():\n    cleaned_file_path = f\"cleaned_{name.lower().replace(' ', '_')}.csv\"\n    df.to_csv(cleaned_file_path, index=False)\n    print(f\"Cleaned {name} dataset saved to {cleaned_file_path}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:05.010504Z","iopub.execute_input":"2024-11-13T15:25:05.011323Z","iopub.status.idle":"2024-11-13T15:25:08.785615Z","shell.execute_reply.started":"2024-11-13T15:25:05.011281Z","shell.execute_reply":"2024-11-13T15:25:08.784735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Relevant Columns for Each Dataset**","metadata":{}},{"cell_type":"code","source":"# Define necessary columns for each dataset\nrequired_columns = {\n    'Orders': ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp'],\n    'Order Items': ['order_id', 'product_id', 'seller_id', 'price', 'freight_value'],\n    'Order Payments': ['order_id', 'payment_type', 'payment_value'],\n    'Products': ['product_id', 'product_category_name'],\n    'Sellers': ['seller_id', 'seller_city', 'seller_state'],\n    'Order Reviews': ['order_id', 'review_score'],\n    'Category Translation': ['product_category_name', 'product_category_name_english']\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:12.204332Z","iopub.execute_input":"2024-11-13T15:25:12.205270Z","iopub.status.idle":"2024-11-13T15:25:12.211176Z","shell.execute_reply.started":"2024-11-13T15:25:12.205217Z","shell.execute_reply":"2024-11-13T15:25:12.210342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Filter Each Dataset for Necessary Columns**","metadata":{}},{"cell_type":"code","source":"# Dictionary to store filtered DataFrames\nfiltered_dataframes = {}\n\nfor name, columns in required_columns.items():\n    # Select only the required columns from each cleaned DataFrame\n    filtered_dataframes[name] = cleaned_dataframes[name][columns]\n    print(f\"Filtered columns for '{name}': {filtered_dataframes[name].columns.tolist()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:16.191002Z","iopub.execute_input":"2024-11-13T15:25:16.191367Z","iopub.status.idle":"2024-11-13T15:25:16.215612Z","shell.execute_reply.started":"2024-11-13T15:25:16.191331Z","shell.execute_reply":"2024-11-13T15:25:16.214645Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Merge the Filtered Datasets Dynamically**","metadata":{}},{"cell_type":"code","source":"# Define the merge sequence dynamically: (DataFrame name, key to merge on)\nmerge_sequence = [\n    ('Orders', 'order_id'),\n    ('Order Items', 'order_id'),\n    ('Order Payments', 'order_id'),\n    ('Products', 'product_id'),\n    ('Category Translation', 'product_category_name'),\n    ('Sellers', 'seller_id'),\n    ('Order Reviews', 'order_id')\n]\n\n# Initialize the base DataFrame with the first DataFrame in the sequence\nbase_df_name, base_key = merge_sequence[0]\nbase_df = filtered_dataframes[base_df_name]\nprint(f\"Starting with base DataFrame: {base_df_name}\")\n\n# Sequentially merge each filtered DataFrame\nfor df_name, key in merge_sequence[1:]:\n    print(f\"Merging '{df_name}' into '{base_df_name}' on key '{key}'\")\n    base_df = pd.merge(base_df, filtered_dataframes[df_name], on=key, how='left')\n    print(f\"Shape after merging '{df_name}': {base_df.shape}\")\n\n# Final merged DataFrame\nfinal_merged_df = base_df\nprint(f\"\\nFinal merged DataFrame shape: {final_merged_df.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:19.642500Z","iopub.execute_input":"2024-11-13T15:25:19.643307Z","iopub.status.idle":"2024-11-13T15:25:20.204184Z","shell.execute_reply.started":"2024-11-13T15:25:19.643265Z","shell.execute_reply":"2024-11-13T15:25:20.203219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Verification and Final Check**","metadata":{}},{"cell_type":"code","source":"# Display the first few rows of the final merged DataFrame\nprint(final_merged_df.head())\n\n# Check for any remaining missing values\nmissing_values = final_merged_df.isnull().sum()\nprint(\"\\nMissing values in the final merged DataFrame:\\n\", missing_values[missing_values > 0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:22.573385Z","iopub.execute_input":"2024-11-13T15:25:22.574362Z","iopub.status.idle":"2024-11-13T15:25:22.706082Z","shell.execute_reply.started":"2024-11-13T15:25:22.574321Z","shell.execute_reply":"2024-11-13T15:25:22.705124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**POST MERGE QUALITY CHECKS**","metadata":{}},{"cell_type":"markdown","source":"*Missing Values*","metadata":{}},{"cell_type":"code","source":"missing_values = final_merged_df.isnull().sum()\nprint(\"Missing values per column:\")\nprint(missing_values[missing_values > 0])\n\ntotal_rows = final_merged_df.shape[0]\nmissing_percentage = (missing_values / total_rows) * 100\nprint(\"Percentage of missing values per column:\")\nprint(missing_percentage[missing_percentage > 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:26.583458Z","iopub.execute_input":"2024-11-13T15:25:26.584059Z","iopub.status.idle":"2024-11-13T15:25:26.711241Z","shell.execute_reply.started":"2024-11-13T15:25:26.584019Z","shell.execute_reply":"2024-11-13T15:25:26.710267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Dropping columns with more than 50% missing values\nthreshold = 50.0  # percentage\ncolumns_to_drop = missing_percentage[missing_percentage > threshold].index\n\n# Check and drop columns if any exceed the threshold\nif len(columns_to_drop) > 0:\n    print(f\"Columns to drop (more than {threshold}% missing values): {list(columns_to_drop)}\")\n    final_merged_df.drop(columns=columns_to_drop, inplace=True)\nelse:\n    print(\"No columns exceed the missing value threshold.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:29.458521Z","iopub.execute_input":"2024-11-13T15:25:29.458921Z","iopub.status.idle":"2024-11-13T15:25:29.465720Z","shell.execute_reply.started":"2024-11-13T15:25:29.458882Z","shell.execute_reply":"2024-11-13T15:25:29.464730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the threshold: number of columns minus allowed missing values\nthreshold = final_merged_df.shape[1] - 2  # Allow up to 2 missing values per row\n\n# Drop rows with missing values exceeding the threshold\ncleaned_df = final_merged_df.dropna(thresh=threshold)\n\n# Display the number of rows removed\nrows_removed = final_merged_df.shape[0] - cleaned_df.shape[0]\nprint(f\"Number of rows removed: {rows_removed}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:31.562164Z","iopub.execute_input":"2024-11-13T15:25:31.562507Z","iopub.status.idle":"2024-11-13T15:25:31.730832Z","shell.execute_reply.started":"2024-11-13T15:25:31.562473Z","shell.execute_reply":"2024-11-13T15:25:31.729891Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Validate Datatypes*","metadata":{}},{"cell_type":"code","source":"print(\"Data types before conversion:\")\nprint(final_merged_df.dtypes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:33.766156Z","iopub.execute_input":"2024-11-13T15:25:33.767006Z","iopub.status.idle":"2024-11-13T15:25:33.772773Z","shell.execute_reply.started":"2024-11-13T15:25:33.766965Z","shell.execute_reply":"2024-11-13T15:25:33.771780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Converting date columns to datetime\ndate_columns = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n                'order_delivered_customer_date', 'order_estimated_delivery_date']\nfor col in date_columns:\n    if col in final_merged_df.columns:\n        final_merged_df[col] = pd.to_datetime(final_merged_df[col], errors='coerce')\nprint(\"Data types after conversion:\")\nprint(final_merged_df.dtypes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:36.182036Z","iopub.execute_input":"2024-11-13T15:25:36.182471Z","iopub.status.idle":"2024-11-13T15:25:36.232690Z","shell.execute_reply.started":"2024-11-13T15:25:36.182431Z","shell.execute_reply":"2024-11-13T15:25:36.231761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Duplicate Values*","metadata":{}},{"cell_type":"code","source":"duplicate_rows = final_merged_df[final_merged_df.duplicated(keep=False)]\nif not duplicate_rows.empty:\n    print(f\"Warning: Found {duplicate_rows.shape[0]} duplicate rows.\")\nelse:\n    print(\"No duplicate rows found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:38.774368Z","iopub.execute_input":"2024-11-13T15:25:38.774971Z","iopub.status.idle":"2024-11-13T15:25:38.973450Z","shell.execute_reply.started":"2024-11-13T15:25:38.774931Z","shell.execute_reply":"2024-11-13T15:25:38.972457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_merged_df.drop_duplicates(inplace=True)\nprint(\"Duplicate rows removed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:40.901657Z","iopub.execute_input":"2024-11-13T15:25:40.902047Z","iopub.status.idle":"2024-11-13T15:25:41.117218Z","shell.execute_reply.started":"2024-11-13T15:25:40.902009Z","shell.execute_reply":"2024-11-13T15:25:41.116286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Data Consistency*","metadata":{}},{"cell_type":"markdown","source":"Numerical Variables","metadata":{}},{"cell_type":"code","source":"# Define expected ranges for numerical columns\nexpected_ranges = {\n    'price': (0, None),  # No negative prices, upper limit None (unbounded)\n    'freight_value': (0, None),  # Freight value should be non-negative\n    'payment_value': (0, None),  # Payment values should be non-negative\n    'review_score': (1, 5)  # Assuming review scores are between 1 and 5\n}\n\n# Loop through numerical columns and validate their ranges\nfor col, dtype in final_merged_df.dtypes.items():\n    if dtype == 'float64' or dtype == 'int64':  # Identifying numerical columns\n        if col in expected_ranges:\n            min_val, max_val = expected_ranges[col]\n            \n            # Check for values below the expected minimum\n            if min_val is not None:\n                below_min = final_merged_df[final_merged_df[col] < min_val]\n                if not below_min.empty:\n                    print(f\"Warning: Found {below_min.shape[0]} entries in '{col}' below the minimum expected value of {min_val}. Setting them to {min_val}.\")\n                    final_merged_df.loc[final_merged_df[col] < min_val, col] = min_val\n            \n            # Check for values above the expected maximum\n            if max_val is not None:\n                above_max = final_merged_df[final_merged_df[col] > max_val]\n                if not above_max.empty:\n                    print(f\"Warning: Found {above_max.shape[0]} entries in '{col}' above the maximum expected value of {max_val}. Setting them to {max_val}.\")\n                    final_merged_df.loc[final_merged_df[col] > max_val, col] = max_val\n        else:\n            print(f\"No expected range specified for '{col}'. Please review this column manually if needed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:43.933393Z","iopub.execute_input":"2024-11-13T15:25:43.934010Z","iopub.status.idle":"2024-11-13T15:25:43.946582Z","shell.execute_reply.started":"2024-11-13T15:25:43.933968Z","shell.execute_reply":"2024-11-13T15:25:43.945734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Categorical Variables","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Columns that require standardization (categorical text columns)\ncategorical_columns = [\n    'order_status', 'payment_type', \n    'product_category_name', 'product_category_name_english', \n    'seller_city', 'seller_state'\n]\n\n# Function to standardize categorical data\ndef standardize_categorical_data(df, columns):\n    for col in columns:\n        if col in df.columns:\n            # Convert to lowercase, replace spaces with underscores, and remove special characters\n            df[col] = df[col].str.lower().str.replace(' ', '_').str.replace(r'[^a-z0-9_]', '', regex=True)\n    return df\n\n# Applying the function to standardize selected categorical columns\nfinal_merged_df = standardize_categorical_data(final_merged_df, categorical_columns)\n\n# Verify the results\nprint(\"Standardized Categorical Columns:\")\nfor col in categorical_columns:\n    print(f\"{col} unique values after standardization:\\n{final_merged_df[col].unique()}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:47.153057Z","iopub.execute_input":"2024-11-13T15:25:47.153899Z","iopub.status.idle":"2024-11-13T15:25:48.187517Z","shell.execute_reply.started":"2024-11-13T15:25:47.153855Z","shell.execute_reply":"2024-11-13T15:25:48.186516Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Mapping if needed for categorical variables  ------- **CAUTION ---**","metadata":{}},{"cell_type":"code","source":"import sys\n!{sys.executable} -m pip install --upgrade packaging\n\npip install --upgrade skrub\n\nimport sys\n!{sys.executable} -m pip show packaging\n\n!{sys.executable} -m pip show skrub\n\n\n\n!{sys.executable} -m pip uninstall skrub -y packaging\n\npip install skrub packaging\n\nimport pandas as pd\nfrom skrub import SimilarityEncoder\n\n# Initialize the SimilarityEncoder\nencoder = SimilarityEncoder(similarity='ngram', ngram_range=(2, 4), categories='auto')\n\n# Function to encode and replace categorical columns\ndef encode_categorical_columns(df, columns):\n    for col in columns:\n        if col in df.columns:\n            # Reshape the column to a 2D array as required by the encoder\n            col_data = df[[col]].astype(str).values\n            # Fit and transform the data\n            encoded_data = encoder.fit_transform(col_data)\n            # Create a DataFrame with the encoded data\n            encoded_df = pd.DataFrame(encoded_data, index=df.index)\n            # Rename columns to reflect the original column name\n            encoded_df.columns = [f\"{col}_encoded_{i}\" for i in range(encoded_df.shape[1])]\n            # Drop the original column and concatenate the encoded columns\n            df = df.drop(columns=[col]).join(encoded_df)\n    return df\n\n# Apply the encoding to the DataFrame\nfinal_merged_df = encode_categorical_columns(final_merged_df, categorical_columns)\n\n\n#checking for consistency\nfor col in categorical_columns:\n    encoded_cols = [c for c in final_merged_df.columns if c.startswith(f\"{col}_encoded_\")]\n    if encoded_cols:\n        print(f\"Column '{col}' has been encoded into {len(encoded_cols)} columns.\")\n    else:\n        print(f\"Warning: Column '{col}' was not found or encoded.\")\n\n#review after encoding\n# Display the first few rows of the encoded DataFrame\nprint(final_merged_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:02:49.296596Z","iopub.execute_input":"2024-11-13T15:02:49.297018Z","iopub.status.idle":"2024-11-13T15:02:49.345540Z","shell.execute_reply.started":"2024-11-13T15:02:49.296974Z","shell.execute_reply":"2024-11-13T15:02:49.343860Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Referential Integrity Checks*","metadata":{}},{"cell_type":"code","source":"# Example: Checking for orphaned 'product_id' entries\nif 'product_id' in final_merged_df.columns:\n    unique_product_ids = final_merged_df['product_id'].unique()\n    # Assuming 'products_df' is the original products DataFrame\n    missing_products = set(unique_product_ids) - set(filtered_dataframes['Products']['product_id'].unique())\n    if missing_products:\n        print(f\"Warning: Found {len(missing_products)} 'product_id' entries without matching records in products data.\")\n    else:\n        print(\"All 'product_id' entries have matching records in products data.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:25:59.128089Z","iopub.execute_input":"2024-11-13T15:25:59.129040Z","iopub.status.idle":"2024-11-13T15:25:59.166879Z","shell.execute_reply.started":"2024-11-13T15:25:59.129000Z","shell.execute_reply":"2024-11-13T15:25:59.165954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Removing rows with missing 'product_id' references\nif 'product_id' in final_merged_df.columns:\n    final_merged_df = final_merged_df[final_merged_df['product_id'].isin(filtered_dataframes['Products']['product_id'])]\n    print(\"Removed rows with orphaned 'product_id' references.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:26:03.902127Z","iopub.execute_input":"2024-11-13T15:26:03.902511Z","iopub.status.idle":"2024-11-13T15:26:03.952065Z","shell.execute_reply.started":"2024-11-13T15:26:03.902473Z","shell.execute_reply":"2024-11-13T15:26:03.951141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Statistical Summaries*","metadata":{}},{"cell_type":"code","source":"print(\"Descriptive statistics for numerical columns:\")\nprint(final_merged_df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:26:07.382733Z","iopub.execute_input":"2024-11-13T15:26:07.383112Z","iopub.status.idle":"2024-11-13T15:26:07.440799Z","shell.execute_reply.started":"2024-11-13T15:26:07.383075Z","shell.execute_reply":"2024-11-13T15:26:07.439757Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Frequency Distribution*","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Example: Plotting histograms for key numerical columns to visualize distributions\nnumerical_columns = final_merged_df.select_dtypes(include=['float64', 'int64']).columns\n\nfor col in numerical_columns:\n    plt.figure(figsize=(8, 4))\n    plt.hist(final_merged_df[col].dropna(), bins=30, edgecolor='k', alpha=0.7)\n    plt.title(f'Distribution of {col}')\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:26:12.340579Z","iopub.execute_input":"2024-11-13T15:26:12.340968Z","iopub.status.idle":"2024-11-13T15:26:13.679712Z","shell.execute_reply.started":"2024-11-13T15:26:12.340931Z","shell.execute_reply":"2024-11-13T15:26:13.678751Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Validate Business Logic*","metadata":{}},{"cell_type":"code","source":"final_merged_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:26:19.528446Z","iopub.execute_input":"2024-11-13T15:26:19.528856Z","iopub.status.idle":"2024-11-13T15:26:19.535479Z","shell.execute_reply.started":"2024-11-13T15:26:19.528796Z","shell.execute_reply":"2024-11-13T15:26:19.534568Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CAUTION -- Date Columns**","metadata":{}},{"cell_type":"code","source":"# Example: Checking logical date relationships\nif 'order_approved_at' in final_merged_df.columns and 'order_delivered_customer_date' in final_merged_df.columns:\n    invalid_dates = final_merged_df[final_merged_df['order_delivered_customer_date'] < final_merged_df['order_approved_at']]\n    if not invalid_dates.empty:\n        print(f\"Warning: Found {invalid_dates.shape[0]} records with 'order_delivered_customer_date' earlier than 'order_approved_at'.\")\n    else:\n        print(\"All date relationships are valid.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:26:23.706768Z","iopub.execute_input":"2024-11-13T15:26:23.707485Z","iopub.status.idle":"2024-11-13T15:26:23.712774Z","shell.execute_reply.started":"2024-11-13T15:26:23.707447Z","shell.execute_reply":"2024-11-13T15:26:23.711713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Validating 'order_status' values\nif 'order_status' in final_merged_df.columns:\n    valid_statuses = ['delivered', 'shipped', 'canceled', 'processing']\n    invalid_statuses = final_merged_df[~final_merged_df['order_status'].isin(valid_statuses)]\n    if not invalid_statuses.empty:\n        print(f\"Warning: Found {invalid_statuses.shape[0]} entries with unexpected 'order_status' values.\")\n    else:\n        print(\"All 'order_status' values are valid.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:26:25.988597Z","iopub.execute_input":"2024-11-13T15:26:25.989483Z","iopub.status.idle":"2024-11-13T15:26:26.008670Z","shell.execute_reply.started":"2024-11-13T15:26:25.989443Z","shell.execute_reply":"2024-11-13T15:26:26.007773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Saving the Final Dataset**","metadata":{}},{"cell_type":"code","source":"# Save the DataFrame to a CSV file\nfinal_merged_df.to_csv('final_merged_df.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:33:17.279317Z","iopub.execute_input":"2024-11-13T15:33:17.279688Z","iopub.status.idle":"2024-11-13T15:33:19.009647Z","shell.execute_reply.started":"2024-11-13T15:33:17.279649Z","shell.execute_reply":"2024-11-13T15:33:19.008832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_merged_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:34:28.442054Z","iopub.execute_input":"2024-11-13T15:34:28.442775Z","iopub.status.idle":"2024-11-13T15:34:28.448402Z","shell.execute_reply.started":"2024-11-13T15:34:28.442736Z","shell.execute_reply":"2024-11-13T15:34:28.447547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Dataset Basic Information\n\n# Display the first few rows\nprint(\"First few rows of the dataset:\")\nprint(final_merged_df.head())\n\n# Display basic information\nprint(\"\\nDataset Information:\")\nprint(final_merged_df.info())\n\n# Display summary statistics for numerical columns\nprint(\"\\nSummary Statistics:\")\nprint(final_merged_df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T15:08:19.675242Z","iopub.execute_input":"2024-11-13T15:08:19.676147Z","iopub.status.idle":"2024-11-13T15:08:19.862178Z","shell.execute_reply.started":"2024-11-13T15:08:19.676097Z","shell.execute_reply":"2024-11-13T15:08:19.861061Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Exploratory Data Analysis**","metadata":{}},{"cell_type":"code","source":"#Importing Libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:06:09.904534Z","iopub.execute_input":"2024-11-13T18:06:09.904954Z","iopub.status.idle":"2024-11-13T18:06:14.281462Z","shell.execute_reply.started":"2024-11-13T18:06:09.904911Z","shell.execute_reply":"2024-11-13T18:06:14.280585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Reading the final df CSV file\n\nfinal_merged_df = pd.read_csv('/kaggle/input/final-merged-dataset/final_merged_df.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:06:16.912326Z","iopub.execute_input":"2024-11-13T18:06:16.913197Z","iopub.status.idle":"2024-11-13T18:06:17.651803Z","shell.execute_reply.started":"2024-11-13T18:06:16.913150Z","shell.execute_reply":"2024-11-13T18:06:17.650948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Dataset Basic Information\n\n# Display the first few rows\nprint(\"First few rows of the dataset:\")\nprint(final_merged_df.head())\n\n# Display basic information\nprint(\"\\nDataset Information:\")\nprint(final_merged_df.info())\n\n# Display summary statistics for numerical columns\nprint(\"\\nSummary Statistics:\")\nprint(final_merged_df.describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:06:20.581201Z","iopub.execute_input":"2024-11-13T18:06:20.581555Z","iopub.status.idle":"2024-11-13T18:06:20.769437Z","shell.execute_reply.started":"2024-11-13T18:06:20.581522Z","shell.execute_reply":"2024-11-13T18:06:20.768339Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Checking Missing Values\n\n\n# Calculate the number and percentage of missing values per column\nmissing_values = final_merged_df.isnull().sum()\nmissing_percentage = (missing_values / final_merged_df.shape[0]) * 100\n\n# Combine into a DataFrame for better readability\nmissing_data = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage': missing_percentage\n})\n\n# Filter columns with missing values\nmissing_data = missing_data[missing_data['Missing Values'] > 0]\n\nprint(\"\\nMissing Values per Column:\")\nprint(missing_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:06:25.726187Z","iopub.execute_input":"2024-11-13T18:06:25.727020Z","iopub.status.idle":"2024-11-13T18:06:25.842800Z","shell.execute_reply.started":"2024-11-13T18:06:25.726978Z","shell.execute_reply":"2024-11-13T18:06:25.841823Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Categorical*","metadata":{}},{"cell_type":"code","source":"#Distribution of Categorical Variables\n\n# Identify categorical columns\ncategorical_columns = final_merged_df.select_dtypes(include=['object']).columns\n\n# Display number of unique values in the categorical columns\nfor col in categorical_columns:\n    unique_values = final_merged_df[col].nunique()\n    print(f\"'{col}' has {unique_values} unique categories.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:21:25.139417Z","iopub.execute_input":"2024-11-13T18:21:25.140318Z","iopub.status.idle":"2024-11-13T18:21:25.317787Z","shell.execute_reply.started":"2024-11-13T18:21:25.140274Z","shell.execute_reply":"2024-11-13T18:21:25.316797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport math\n\n# List of columns to analyze\ncolumns_to_analyze = ['order_status', 'payment_type', 'product_category_name_english', 'seller_city', 'seller_state']\n\n# Define the number of top categories to display\ntop_n = 10\n\n# Number of plots per row\nplots_per_row = 2\n\n# Calculate the number of rows needed\nnum_rows = math.ceil(len(columns_to_analyze) / plots_per_row)\n\n# Create a figure with the calculated number of subplots\nfig, axes = plt.subplots(num_rows, plots_per_row, figsize=(15, num_rows * 5))\n\n# Flatten the axes array for easy iteration\naxes = axes.flatten()\n\n# Iterate over each specified column and corresponding subplot axis\nfor idx, col in enumerate(columns_to_analyze):\n    if col in final_merged_df.columns:\n        # Calculate the top N categories\n        top_categories = final_merged_df[col].value_counts(dropna=False).nlargest(top_n)\n        \n        # Plot the distribution of the top N categories\n        ax = axes[idx]\n        top_categories.plot(kind='bar', ax=ax)\n        ax.set_title(f'Top {top_n} Categories in {col}')\n        ax.set_xlabel(col)\n        ax.set_ylabel('Count')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        ax.grid(True)\n    else:\n        print(f\"Column '{col}' not found in the DataFrame.\")\n\n# Remove any unused subplots\nfor j in range(idx + 1, len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:02:26.113945Z","iopub.execute_input":"2024-11-13T19:02:26.114799Z","iopub.status.idle":"2024-11-13T19:02:27.436896Z","shell.execute_reply.started":"2024-11-13T19:02:26.114749Z","shell.execute_reply":"2024-11-13T19:02:27.435974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Numerical*","metadata":{}},{"cell_type":"code","source":"#Distribution of Numerical Variables\n\n# Identify numerical columns\nnumerical_columns = final_merged_df.select_dtypes(include=['number']).columns\n\n# Plot histograms for numerical columns\nfinal_merged_df[numerical_columns].hist(figsize=(15, 10), bins=30, edgecolor='black')\nplt.suptitle('Histograms of Numerical Features')\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:28:12.537363Z","iopub.execute_input":"2024-11-13T18:28:12.538236Z","iopub.status.idle":"2024-11-13T18:28:13.946459Z","shell.execute_reply.started":"2024-11-13T18:28:12.538192Z","shell.execute_reply":"2024-11-13T18:28:13.945534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\n# Number of plots per row\nplots_per_row = 2\n\n# Filtered lists of columns for manageable plots\nnumerical_columns = final_merged_df.select_dtypes(include=['float64', 'int64']).columns\ncategorical_columns = [col for col in final_merged_df.select_dtypes(include=['object']).columns if final_merged_df[col].nunique() <= 20]\n\n# Calculate the total number of plots needed\ntotal_plots = len(numerical_columns) * len(categorical_columns)\nnum_rows = math.ceil(total_plots / plots_per_row)\n\n# Set up figure with calculated number of rows\nfig, axes = plt.subplots(num_rows, plots_per_row, figsize=(15, num_rows * 5))\naxes = axes.flatten()  # Flatten axes for easier indexing\n\n# Initialize plot index\nplot_idx = 0\n\n# Generate box plots\nfor num_col in numerical_columns:\n    for cat_col in categorical_columns:\n        if plot_idx < total_plots:\n            sns.boxplot(data=final_merged_df, x=cat_col, y=num_col, ax=axes[plot_idx])\n            axes[plot_idx].set_title(f'{num_col} Distribution by {cat_col}')\n            axes[plot_idx].set_xlabel(cat_col)\n            axes[plot_idx].set_ylabel(num_col)\n            axes[plot_idx].tick_params(axis='x', rotation=45)\n            axes[plot_idx].grid(True)\n            plot_idx += 1\n\n# Remove any unused subplots if total_plots < len(axes)\nfor j in range(plot_idx, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:20:13.360613Z","iopub.execute_input":"2024-11-13T19:20:13.361482Z","iopub.status.idle":"2024-11-13T19:20:16.090573Z","shell.execute_reply.started":"2024-11-13T19:20:13.361442Z","shell.execute_reply":"2024-11-13T19:20:16.089644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Correlation Matrix*","metadata":{}},{"cell_type":"code","source":"# Compute correlation matrix\ncorrelation_matrix = final_merged_df[numerical_columns].corr()\n\n# Plot heatmap of the correlation matrix\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Matrix of Numerical Variables')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:40:23.580468Z","iopub.execute_input":"2024-11-13T18:40:23.581044Z","iopub.status.idle":"2024-11-13T18:40:23.986963Z","shell.execute_reply.started":"2024-11-13T18:40:23.581001Z","shell.execute_reply":"2024-11-13T18:40:23.985978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Time Series Analysis*","metadata":{}},{"cell_type":"code","source":"#Analyse Trends over time\n\n# Ensure 'order_purchase_timestamp' is in datetime format\nfinal_merged_df['order_purchase_timestamp'] = pd.to_datetime(final_merged_df['order_purchase_timestamp'])\n\n# Extract date-related features\nfinal_merged_df['order_date'] = final_merged_df['order_purchase_timestamp'].dt.date\nfinal_merged_df['order_month'] = final_merged_df['order_purchase_timestamp'].dt.to_period('M')\n\n# Plot order counts over time\norder_counts = final_merged_df['order_date'].value_counts().sort_index()\nplt.figure(figsize=(12, 6))\norder_counts.plot()\nplt.title('Number of Orders Over Time')\nplt.xlabel('Date')\nplt.ylabel('Number of Orders')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T14:01:08.983210Z","iopub.execute_input":"2024-11-13T14:01:08.984351Z","iopub.status.idle":"2024-11-13T14:01:09.377436Z","shell.execute_reply.started":"2024-11-13T14:01:08.984239Z","shell.execute_reply":"2024-11-13T14:01:09.376322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure 'order_purchase_timestamp' is in datetime format\nfinal_merged_df['order_purchase_timestamp'] = pd.to_datetime(final_merged_df['order_purchase_timestamp'])\n\n# Set the order purchase timestamp as the index\ntime_series_df = final_merged_df.set_index('order_purchase_timestamp')\n\n# Resample to get monthly order counts\nmonthly_orders = time_series_df['order_id'].resample('M').count()\n\n# Plot the time series\nplt.figure(figsize=(12, 6))\nmonthly_orders.plot()\nplt.title('Monthly Order Counts between 2017-10 and 2018-07')\nplt.xlabel('Date')\nplt.ylabel('Number of Orders')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:05:32.357871Z","iopub.execute_input":"2024-11-13T19:05:32.358606Z","iopub.status.idle":"2024-11-13T19:05:32.834730Z","shell.execute_reply.started":"2024-11-13T19:05:32.358565Z","shell.execute_reply":"2024-11-13T19:05:32.833742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Pairwise Relationships*","metadata":{}},{"cell_type":"code","source":"# Select a subset of numerical columns for pairplot\nsubset_numerical_columns = numerical_columns[:5]  # Adjust the number as needed\n\n# Plot pairplot\nsns.pairplot(final_merged_df[subset_numerical_columns].dropna(), height=1.5)\nplt.suptitle('Pairwise Relationships Between Numerical Variables', y=1.02)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:28:21.763501Z","iopub.execute_input":"2024-11-13T19:28:21.763929Z","iopub.status.idle":"2024-11-13T19:28:37.251449Z","shell.execute_reply.started":"2024-11-13T19:28:21.763889Z","shell.execute_reply":"2024-11-13T19:28:37.250526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Geographical Trends* ","metadata":{}},{"cell_type":"code","source":"#Analysing Geographical Data\n\n# Plot the distribution of orders by state\nplt.figure(figsize=(12, 6))\nfinal_merged_df['seller_state'].value_counts().plot(kind='bar')\nplt.title('Number of Orders by Seller State')\nplt.xlabel('State')\nplt.ylabel('Number of Orders')\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T14:01:52.961817Z","iopub.execute_input":"2024-11-13T14:01:52.962830Z","iopub.status.idle":"2024-11-13T14:01:53.361134Z","shell.execute_reply.started":"2024-11-13T14:01:52.962785Z","shell.execute_reply":"2024-11-13T14:01:53.360074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the script file and write the content to it\nscript_content = \"\"\"\n#!/bin/bash\n\n# Set Git user information\ngit config --global user.email \"chennarajvamsi@outlook.com\"\ngit config --global user.name \"Rajvamsi Chenna\"\n\n# GitHub Username and Repository\nGITHUB_USERNAME=\"rajvamsi18\"\nREPO_NAME=\"Real-Time-Pricing-Algorithm-Project\"\n\n# Get the GitHub Personal Access Token from Kaggle secrets\nGITHUB_TOKEN=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\n\n# Clone the repository (only if not already cloned)\nif [ ! -d \"$REPO_NAME\" ]; then\n    echo \"Cloning the repository...\"\n    git clone https://$GITHUB_USERNAME:$GITHUB_TOKEN@github.com/$GITHUB_USERNAME/$REPO_NAME.git\nfi\n\n# Change to the repository directory\ncd $REPO_NAME\n\n# Pull the latest changes from GitHub\necho \"Pulling the latest changes from GitHub...\"\ngit pull origin main\n\n# Stage all changes\necho \"Staging changes...\"\ngit add .\n\n# Commit changes with a default or user-defined message\nCOMMIT_MESSAGE=${1:-\"Automated commit from Kaggle\"}\necho \"Committing changes with message: $COMMIT_MESSAGE\"\ngit commit -m \"$COMMIT_MESSAGE\"\n\n# Push changes back to GitHub\necho \"Pushing changes to GitHub...\"\ngit push https://$GITHUB_USERNAME:$GITHUB_TOKEN@github.com/$GITHUB_USERNAME/$REPO_NAME.git\n\"\"\"\n\n# Write the script content to a file\nwith open(\"sync_to_github.sh\", \"w\") as file:\n    file.write(script_content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T21:21:52.709795Z","iopub.execute_input":"2024-11-13T21:21:52.710821Z","iopub.status.idle":"2024-11-13T21:21:52.717356Z","shell.execute_reply.started":"2024-11-13T21:21:52.710760Z","shell.execute_reply":"2024-11-13T21:21:52.716416Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"!chmod +x sync_to_github.sh\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T21:21:55.596585Z","iopub.execute_input":"2024-11-13T21:21:55.597218Z","iopub.status.idle":"2024-11-13T21:21:56.618292Z","shell.execute_reply.started":"2024-11-13T21:21:55.597178Z","shell.execute_reply":"2024-11-13T21:21:56.617077Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"!./sync_to_github.sh \"Version 1 EDA\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T21:21:59.389939Z","iopub.execute_input":"2024-11-13T21:21:59.390361Z","iopub.status.idle":"2024-11-13T21:22:01.069837Z","shell.execute_reply.started":"2024-11-13T21:21:59.390312Z","shell.execute_reply":"2024-11-13T21:22:01.068799Z"}},"outputs":[{"name":"stdout","text":"Pulling the latest changes from GitHub...\nFrom https://github.com/rajvamsi18/Real-Time-Pricing-Algorithm-Project\n * branch            main       -> FETCH_HEAD\nAlready up to date.\nStaging changes...\nCommitting changes with message: Version 1 EDA\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\nPushing changes to GitHub...\nEverything up-to-date\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"!ls Real-Time-Pricing-Algorithm-Project\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T21:26:44.044757Z","iopub.execute_input":"2024-11-13T21:26:44.045260Z","iopub.status.idle":"2024-11-13T21:26:45.054637Z","shell.execute_reply.started":"2024-11-13T21:26:44.045212Z","shell.execute_reply":"2024-11-13T21:26:45.053397Z"}},"outputs":[{"name":"stdout","text":"README.md  data  notebooks\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"!mv real-time-pricing.ipynb Real-Time-Pricing-Algorithm-Project/\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T21:27:48.122044Z","iopub.execute_input":"2024-11-13T21:27:48.122796Z","iopub.status.idle":"2024-11-13T21:27:49.169209Z","shell.execute_reply.started":"2024-11-13T21:27:48.122739Z","shell.execute_reply":"2024-11-13T21:27:49.168213Z"}},"outputs":[{"name":"stdout","text":"mv: cannot stat 'real-time-pricing.ipynb': No such file or directory\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"*Feature Engineering*","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}