{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2669146,"sourceType":"datasetVersion","datasetId":55151},{"sourceId":9896478,"sourceType":"datasetVersion","datasetId":6078755},{"sourceId":10088875,"sourceType":"datasetVersion","datasetId":6220794},{"sourceId":10130999,"sourceType":"datasetVersion","datasetId":6252421},{"sourceId":10131797,"sourceType":"datasetVersion","datasetId":6253028},{"sourceId":10132123,"sourceType":"datasetVersion","datasetId":6253269}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:41:55.002367Z","iopub.execute_input":"2024-12-03T13:41:55.002643Z","iopub.status.idle":"2024-12-03T13:42:00.218561Z","shell.execute_reply.started":"2024-12-03T13:41:55.002615Z","shell.execute_reply":"2024-12-03T13:42:00.217882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Reading the final df CSV file\n\nrefined_features_df = pd.read_csv('/kaggle/input/dynamic-pricing-refined-features/dynamic_pricing_refined_features_DF.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:42:06.040832Z","iopub.execute_input":"2024-12-03T13:42:06.041743Z","iopub.status.idle":"2024-12-03T13:42:06.523835Z","shell.execute_reply.started":"2024-12-03T13:42:06.041707Z","shell.execute_reply":"2024-12-03T13:42:06.522904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"refined_features_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:42:11.828811Z","iopub.execute_input":"2024-12-03T13:42:11.829198Z","iopub.status.idle":"2024-12-03T13:42:11.836388Z","shell.execute_reply.started":"2024-12-03T13:42:11.829162Z","shell.execute_reply":"2024-12-03T13:42:11.835417Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Train-Test Split**","metadata":{}},{"cell_type":"code","source":"# Define features (X) and target (y)\nX = refined_features_df.drop(columns=['regional_demand'])\ny = refined_features_df['regional_demand']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training features shape: {X_train.shape}\")\nprint(f\"Testing features shape: {X_test.shape}\")\nprint(f\"Training target shape: {y_train.shape}\")\nprint(f\"Testing target shape: {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:42:41.201121Z","iopub.execute_input":"2024-12-03T13:42:41.201848Z","iopub.status.idle":"2024-12-03T13:42:41.266508Z","shell.execute_reply.started":"2024-12-03T13:42:41.201817Z","shell.execute_reply":"2024-12-03T13:42:41.265646Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Initialize models**","metadata":{}},{"cell_type":"code","source":"\nmodels = {\n    'Linear Regression': LinearRegression(),\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n    'LightGBM': LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),\n    'CatBoost': CatBoostRegressor(n_estimators=100, random_state=42, verbose=0)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:42:50.644586Z","iopub.execute_input":"2024-12-03T13:42:50.645337Z","iopub.status.idle":"2024-12-03T13:42:50.654533Z","shell.execute_reply.started":"2024-12-03T13:42:50.645300Z","shell.execute_reply":"2024-12-03T13:42:50.653602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluate each model using cross-validation**","metadata":{}},{"cell_type":"code","source":"cv_results = {}\nfor model_name, model in models.items():\n    print(f\"Evaluating {model_name}...\")\n    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n    cv_results[model_name] = {\n        'Mean CV MAE': -np.mean(cv_scores),\n        'Std Dev CV MAE': np.std(cv_scores),\n        'Model': model\n    }\n    print(f\"{model_name} - Mean CV MAE: {-np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n\n# Shortlist models based on CV performance\nshortlisted_models = sorted(cv_results.items(), key=lambda x: x[1]['Mean CV MAE'])[:3]\nprint(\"\\nShortlisted Models for Test Set Evaluation:\", [model[0] for model in shortlisted_models])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:42:54.062323Z","iopub.execute_input":"2024-12-03T13:42:54.062665Z","iopub.status.idle":"2024-12-03T13:43:11.681084Z","shell.execute_reply.started":"2024-12-03T13:42:54.062635Z","shell.execute_reply":"2024-12-03T13:43:11.680018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluate shortlisted models on the test set**","metadata":{}},{"cell_type":"code","source":"\ntest_results = {}\nfor model_name, model_data in shortlisted_models:\n    model = model_data['Model']\n    # Train on the full training set\n    model.fit(X_train, y_train)\n    # Predict on the test set\n    y_pred = model.predict(X_test)\n    # Evaluate metrics\n    metrics = {\n        'MAE': mean_absolute_error(y_test, y_pred),\n        'MSE': mean_squared_error(y_test, y_pred),\n        'R²': r2_score(y_test, y_pred)\n    }\n    test_results[model_name] = metrics\n    print(f\"\\n{model_name} - Test Set Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n\n# Select the best model based on Test Set MAE\nfinal_model_name = min(test_results, key=lambda x: test_results[x]['MAE'])\nfinal_model = models[final_model_name]\n\nprint(f\"\\nFinal Selected Model: {final_model_name}\")\n\n# Save the selected model's test set performance\nfinal_test_metrics = test_results[final_model_name]\nprint(f\"\\nFinal Model Test Set Metrics: {final_test_metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:43:25.642875Z","iopub.execute_input":"2024-12-03T13:43:25.643815Z","iopub.status.idle":"2024-12-03T13:43:27.401935Z","shell.execute_reply.started":"2024-12-03T13:43:25.643776Z","shell.execute_reply":"2024-12-03T13:43:27.401079Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Analyze Overfitting, Feature Importance, and Diagnostics**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Assuming Random Forest is the final model\nfinal_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nfinal_model.fit(X_train, y_train)\n\n# 1. Learning Curve Analysis\ndef plot_learning_curve(model, X, y, cv=5, scoring='neg_mean_absolute_error'):\n    \"\"\"\n    Plots the learning curve for the given model.\n    \"\"\"\n    train_sizes, train_scores, test_scores = learning_curve(\n        model, X, y, cv=cv, scoring=scoring, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10), random_state=42\n    )\n    train_scores_mean = -np.mean(train_scores, axis=1)\n    test_scores_mean = -np.mean(test_scores, axis=1)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_sizes, train_scores_mean, label='Training Error', marker='o')\n    plt.plot(train_sizes, test_scores_mean, label='Cross-Validation Error', marker='o')\n    plt.title('Learning Curve')\n    plt.xlabel('Training Set Size')\n    plt.ylabel('Mean Absolute Error')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nprint(\"\\n--- Learning Curve Analysis ---\")\nplot_learning_curve(final_model, X_train, y_train)\n\n# 2. Feature Importance Analysis (Tree-based model)\ndef plot_feature_importance(model, feature_names, top_n=10):\n    \"\"\"\n    Plots the top N important features for tree-based models.\n    \"\"\"\n    importances = model.feature_importances_\n    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).head(top_n)\n\n    feature_importance_df.plot(kind='barh', x='Feature', y='Importance', figsize=(10, 6), legend=False)\n    plt.title(f\"Top {top_n} Feature Importances\")\n    plt.xlabel('Importance')\n    plt.ylabel('Feature')\n    plt.gca().invert_yaxis()\n    plt.grid(True)\n    plt.show()\n\nprint(\"\\n--- Feature Importance ---\")\nplot_feature_importance(final_model, X_train.columns)\n\n# 3. Check for Data Leakage (Correlation with Target)\ndef check_data_leakage(df, target_column):\n    \"\"\"\n    Checks for high correlations between features and the target, indicating potential leakage.\n    \"\"\"\n    correlation_with_target = df.corr()[target_column].sort_values(ascending=False)\n    print(\"\\n--- Correlation with Target ---\")\n    print(correlation_with_target.head(10))  # Display top 10 correlations\n    return correlation_with_target\n\n# Check correlations in the training data\nprint(\"\\n--- Checking Data Leakage ---\")\ncorrelations = check_data_leakage(pd.concat([X_train, y_train], axis=1), target_column='regional_demand')\n\n# 4. Test Set Evaluation and Residual Analysis\ny_pred_test = final_model.predict(X_test)\n\n# Metrics\ntest_mae = mean_absolute_error(y_test, y_pred_test)\ntest_mse = mean_squared_error(y_test, y_pred_test)\ntest_r2 = r2_score(y_test, y_pred_test)\nprint(f\"\\n--- Test Set Performance ---\")\nprint(f\"MAE: {test_mae:.4f}\")\nprint(f\"MSE: {test_mse:.4f}\")\nprint(f\"R²: {test_r2:.4f}\")\n\n# Residual Analysis\nresiduals = y_test - y_pred_test\n\nplt.figure(figsize=(10, 6))\nplt.scatter(y_pred_test, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.title('Residual Analysis')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:52:26.913354Z","iopub.execute_input":"2024-12-03T13:52:26.913739Z","iopub.status.idle":"2024-12-03T13:53:00.566468Z","shell.execute_reply.started":"2024-12-03T13:52:26.913708Z","shell.execute_reply":"2024-12-03T13:53:00.565539Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Validating and Evaluating Random Forest Model*","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error\nimport shap\nimport matplotlib.pyplot as plt\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:53:36.724528Z","iopub.execute_input":"2024-12-03T13:53:36.724923Z","iopub.status.idle":"2024-12-03T13:53:40.998624Z","shell.execute_reply.started":"2024-12-03T13:53:36.724875Z","shell.execute_reply":"2024-12-03T13:53:40.997914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport shap\nimport matplotlib.pyplot as plt\nimport joblib\n\n# --- Evaluation Function ---\ndef evaluate_model(model, X, y, transform_func=None):\n    predictions = model.predict(X)\n    if transform_func:\n        predictions = transform_func(predictions)\n        y = transform_func(y)\n    mae = mean_absolute_error(y, predictions)\n    mape = np.mean(np.abs((y - predictions) / y)) * 100\n    mse = mean_squared_error(y, predictions)\n    r2 = r2_score(y, predictions)\n    return {'MAE': mae, 'MAPE': mape, 'MSE': mse, 'R²': r2}\n\n# --- Add Noise to Dataset ---\ndef augment_data(X, noise_level=0.02):\n    noisy_X = X.copy()\n    for col in noisy_X.select_dtypes(include=[np.number]).columns:\n        noise = np.random.normal(0, noise_level * noisy_X[col].std(), noisy_X[col].shape)\n        noisy_X[col] += noise\n    return noisy_X\n\n# --- Check Skewness and Transform ---\ndef check_and_transform_target(y):\n    skewness = y.skew()\n    print(f\"Skewness of Target Variable: {skewness:.2f}\")\n    if abs(skewness) > 1:\n        print(\"High skewness detected. Applying log transformation.\")\n        return np.log1p(y), np.expm1\n    print(\"No significant skewness detected. Proceeding with original target.\")\n    return y, None\n\n# --- Dataset Preparation ---\nX = refined_features_df.drop(columns=['regional_demand'])\ny, inverse_transform_func = check_and_transform_target(refined_features_df['regional_demand'])\n\n# Split into train, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nprint(f\"Training Set: {X_train.shape}, Validation Set: {X_val.shape}, Test Set: {X_test.shape}\")\n\n# Augment training data with noise\nX_train_augmented = augment_data(X_train, noise_level=0.02)\n\n# --- Hyperparameter Tuning ---\nparam_grid = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [8, 10, 12],\n    'min_samples_split': [2, 5, 8],\n    'min_samples_leaf': [2, 5]\n}\n\nrf_model = RandomForestRegressor(random_state=42)\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error', verbose=1, n_jobs=-1)\ngrid_search.fit(X_train_augmented, y_train)\n\nbest_rf_model = grid_search.best_estimator_\nprint(f\"Best Random Forest Parameters: {grid_search.best_params_}\")\n\n# --- Evaluate on Validation Set ---\nval_metrics = evaluate_model(best_rf_model, X_val, y_val, transform_func=inverse_transform_func)\nprint(f\"Validation Set Performance: {val_metrics}\")\n\n# --- Residual Analysis ---\npredictions = best_rf_model.predict(X_test)\nif inverse_transform_func:\n    predictions = inverse_transform_func(predictions)\n    y_test = inverse_transform_func(y_test)\n\nresiduals = y_test - predictions\nplt.scatter(predictions, residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.title(\"Residuals vs. Predicted Values (Validation Set)\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()\n\n# --- SHAP Explainability ---\nexplainer = shap.TreeExplainer(best_rf_model)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train, plot_type='bar')\nshap.summary_plot(shap_values, X_train)\n\n# --- Final Test Evaluation ---\ntest_metrics = evaluate_model(best_rf_model, X_test, y_test, transform_func=None)\nprint(f\"Test Set Performance: {test_metrics}\")\n\n# --- Save Model and Metrics ---\njoblib.dump(best_rf_model, 'final_random_forest_model.pkl')\nprint(\"Final model saved as 'final_random_forest_model.pkl'.\")\n\nwith open('model_metrics.txt', 'w') as f:\n    f.write(f\"Validation Metrics: {val_metrics}\\n\")\n    f.write(f\"Test Metrics: {test_metrics}\\n\")\nprint(\"Model metrics saved to 'model_metrics.txt'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:53:43.581959Z","iopub.execute_input":"2024-12-03T13:53:43.583010Z","iopub.status.idle":"2024-12-03T14:42:50.952933Z","shell.execute_reply.started":"2024-12-03T13:53:43.582948Z","shell.execute_reply":"2024-12-03T14:42:50.952038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Importance Visualization\nfeature_importances = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': best_rf_model.feature_importances_\n}).sort_values(by='Importance', ascending=False)\n\n# Plot top 10 features\nfeature_importances[:10].plot(kind='barh', x='Feature', y='Importance', legend=False, figsize=(10, 6))\nplt.title(\"Top 10 Feature Importances\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.gca().invert_yaxis()\nplt.grid()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T14:43:00.123318Z","iopub.execute_input":"2024-12-03T14:43:00.124178Z","iopub.status.idle":"2024-12-03T14:43:00.390351Z","shell.execute_reply.started":"2024-12-03T14:43:00.124136Z","shell.execute_reply":"2024-12-03T14:43:00.389537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}